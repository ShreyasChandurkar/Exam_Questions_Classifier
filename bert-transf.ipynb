{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:25:57.808799Z","iopub.execute_input":"2024-05-20T16:25:57.809657Z","iopub.status.idle":"2024-05-20T16:26:10.504406Z","shell.execute_reply.started":"2024-05-20T16:25:57.809622Z","shell.execute_reply":"2024-05-20T16:26:10.503393Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install PyPDF2\n!pip install langdetect\n!pip install contractions","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:23:16.112083Z","iopub.execute_input":"2024-05-20T16:23:16.112735Z","iopub.status.idle":"2024-05-20T16:23:44.391573Z","shell.execute_reply.started":"2024-05-20T16:23:16.112704Z","shell.execute_reply":"2024-05-20T16:23:44.390539Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=73f2c6f56a2ee24c08a7a3ed1a60d4515bba154ff7e65ecc42f6b94e7a35d7f6\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\nCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\nDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\n\nimport re\nimport string\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\n\n\n# Data preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\nfrom langdetect import detect, LangDetectException\nimport contractions\nfrom nltk.tokenize import word_tokenize\n\nimport PyPDF2\nfrom PyPDF2 import PdfReader\n\nimport time\n\n\n\n\n# Define stop words for text cleaning\nstop_words = set(stopwords.words('english'))\nfrom huggingface_hub import notebook_login\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset, load_metric, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:26:13.406194Z","iopub.execute_input":"2024-05-20T16:26:13.407010Z","iopub.status.idle":"2024-05-20T16:26:13.579274Z","shell.execute_reply.started":"2024-05-20T16:26:13.406968Z","shell.execute_reply":"2024-05-20T16:26:13.578297Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_hub\nfrom huggingface_hub import notebook_login","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:08:56.038891Z","iopub.execute_input":"2024-05-20T16:08:56.039565Z","iopub.status.idle":"2024-05-20T16:09:08.341441Z","shell.execute_reply.started":"2024-05-20T16:08:56.039535Z","shell.execute_reply":"2024-05-20T16:09:08.340230Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"notebook_login()\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:09:16.450079Z","iopub.execute_input":"2024-05-20T16:09:16.450448Z","iopub.status.idle":"2024-05-20T16:09:16.475768Z","shell.execute_reply.started":"2024-05-20T16:09:16.450418Z","shell.execute_reply":"2024-05-20T16:09:16.474816Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb83ba9529d74f8dad850920e47284dd"}},"metadata":{}}]},{"cell_type":"code","source":"# Load the model and tokenizer from Hugging Face Hub\nmodel_id = \"ChetanIngle/Bert-Bloom-toxonomy\"  # Model ID on Hugging Face\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:09:57.061631Z","iopub.execute_input":"2024-05-20T16:09:57.062304Z","iopub.status.idle":"2024-05-20T16:10:07.484131Z","shell.execute_reply.started":"2024-05-20T16:09:57.062271Z","shell.execute_reply":"2024-05-20T16:10:07.483137Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/995 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bf70ff896cb46538d1685cfe89179da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c635748263b640d592c9c16f2c2941ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf18ceff5a0a4fa4aff9e0fae94fe516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca5cdb4f8724f109a6b7ecd0e529e29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b773b57c904925a89fc1fe4cc28dd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58381973b6f24fa8ae3390706d4a5c70"}},"metadata":{}}]},{"cell_type":"code","source":"# Function to make predictions\ndef make_prediction(question,model,tokenizer):\n    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        predictions = torch.argmax(logits, dim=-1)\n\n        label_id = predictions.item()\n\n    return label_id","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:11:37.634165Z","iopub.execute_input":"2024-05-20T16:11:37.634847Z","iopub.status.idle":"2024-05-20T16:11:37.640225Z","shell.execute_reply.started":"2024-05-20T16:11:37.634818Z","shell.execute_reply":"2024-05-20T16:11:37.639307Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:34:43.897647Z","iopub.execute_input":"2024-05-20T16:34:43.898493Z","iopub.status.idle":"2024-05-20T16:34:44.590389Z","shell.execute_reply.started":"2024-05-20T16:34:43.898456Z","shell.execute_reply":"2024-05-20T16:34:44.589376Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Using the function to make a prediction\nquestion = \"What is the capital of France?\"\npredicted_bloom_level = make_prediction(question,model,tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T08:49:37.917395Z","iopub.execute_input":"2024-05-20T08:49:37.917829Z","iopub.status.idle":"2024-05-20T08:49:37.985619Z","shell.execute_reply.started":"2024-05-20T08:49:37.917798Z","shell.execute_reply":"2024-05-20T08:49:37.984309Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"questions = ['1. Match the correct data structure to its time complexity for searching (e.g., Hash Table -',\n '2. Examine the syntax of a lambda function in Python?',\n '3. Discuss the principles of cybersecurity, explaining concepts such as encryption,',\n '4. Compare the different types of file systems (e.g., FAT32, NTFS, ext4) used in operating',\n '5. Solve a hypothetical scenario involving a data breach in a healthcare organization and',\n '6. Analyze the potential risks and benefits of using facial recognition technology for law',\n '7. A transportation agency is planning a road widening project to accommodate increasing',\n '8. Your team is designing a sustainable building envelope for a high -rise office tower to',\n '9. Evaluate: Judge the reliability of a finite element analysis (FEA) model in predicting the',\n '10. Assess the effectiveness of flood mitigation measures in reducing the risk of urban',\n '11. You are tasked with developing a sustainable transportation infrastructure for a',\n '12. You are designing a green building certification system for promoting sustainable']","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:11:49.222863Z","iopub.execute_input":"2024-05-20T16:11:49.223692Z","iopub.status.idle":"2024-05-20T16:11:49.228708Z","shell.execute_reply.started":"2024-05-20T16:11:49.223661Z","shell.execute_reply":"2024-05-20T16:11:49.227648Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"predicted_bloom_level","metadata":{"execution":{"iopub.status.busy":"2024-05-20T08:49:45.027449Z","iopub.execute_input":"2024-05-20T08:49:45.027912Z","iopub.status.idle":"2024-05-20T08:49:45.039872Z","shell.execute_reply.started":"2024-05-20T08:49:45.027878Z","shell.execute_reply":"2024-05-20T08:49:45.037778Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"def strip_all_entities(text):\n    text = re.sub(r'\\r|\\n', ' ', text.lower())  # Replace newline and carriage return with space, and convert to lowercase\n    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  # Remove links and mentions\n    text = re.sub(r'[^\\x00-\\x7f]', '', text)  # Remove non-ASCII characters\n    banned_list = string.punctuation\n    table = str.maketrans('', '', banned_list)\n    text = text.translate(table)\n    text = ' '.join(word for word in text.split() if word not in stop_words)\n    return text\n\n# Clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\ndef clean_hashtags(tweet):\n    # Remove hashtags at the end of the sentence\n    new_tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n   \n    # Remove the # symbol from hashtags in the middle of the sentence\n    new_tweet = re.sub(r'#([\\w-]+)', r'\\1', new_tweet).strip()\n   \n    return new_tweet\n\n# Filter special characters such as & and $ present in some words\ndef filter_chars(text):\n    return ' '.join('' if ('$' in word) or ('&' in word) else word for word in text.split())\n\n# Remove multiple spaces\ndef remove_mult_spaces(text):\n    return re.sub(r\"\\s\\s+\", \" \", text)\n\n# Function to check if the text is in English, and return an empty string if it's not\ndef filter_non_english(text):\n    try:\n        lang = detect(text)\n    except LangDetectException:\n        lang = \"unknown\"\n    return text if lang == \"en\" else \"\"\n\n# Expand contractions\ndef expand_contractions(text):\n    return contractions.fix(text)\n\n# Remove numbers\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\n# Lemmatize words\ndef lemmatize(text):\n    words = word_tokenize(text)\n    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n    return ' '.join(lemmatized_words)\n\n# Remove short words\ndef remove_short_words(text, min_len=2):\n    words = text.split()\n    long_words = [word for word in words if len(word) >= min_len]\n    return ' '.join(long_words)\n\n# Replace elongated words with their base form\ndef replace_elongated_words(text):\n    regex_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n    return re.sub(regex_pattern, r'\\1\\3\\4', text)\n\n# Remove repeated punctuation\ndef remove_repeated_punctuation(text):\n    return re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n\n# Remove extra whitespace\ndef remove_extra_whitespace(text):\n    return ' '.join(text.split())\n\n# def remove_url_shorteners(text):\n#     return re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text)\n\n\ndef remove_spaces_tweets(tweet):\n    return tweet.strip()\n\n# Remove short tweets\ndef remove_short_tweets(tweet, min_words=3):\n    words = tweet.split()\n    return tweet if len(words) >= min_words else \"\"\n\n# Function to call all the cleaning functions in the correct order\ndef clean_tweet(tweet):\n    # tweet = strip_emoji(tweet)\n    tweet = expand_contractions(tweet)\n    tweet = filter_non_english(tweet)\n    tweet = strip_all_entities(tweet)\n    tweet = clean_hashtags(tweet)\n    tweet = filter_chars(tweet)\n    tweet = remove_mult_spaces(tweet)\n    tweet = remove_numbers(tweet)\n    #tweet = lemmatize(tweet)\n    tweet = remove_short_words(tweet)\n    tweet = replace_elongated_words(tweet)\n    tweet = remove_repeated_punctuation(tweet)\n    tweet = remove_extra_whitespace(tweet)\n    # tweet = remove_url_shorteners(tweet)\n    tweet = remove_spaces_tweets(tweet)\n    tweet = remove_short_tweets(tweet)\n    tweet = ' '.join(tweet.split())  # Remove multiple spaces between words\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:27:44.300257Z","iopub.execute_input":"2024-05-20T16:27:44.300673Z","iopub.status.idle":"2024-05-20T16:27:44.321449Z","shell.execute_reply.started":"2024-05-20T16:27:44.300642Z","shell.execute_reply":"2024-05-20T16:27:44.320267Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"output =[]","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:29:01.241621Z","iopub.execute_input":"2024-05-20T16:29:01.242542Z","iopub.status.idle":"2024-05-20T16:29:01.246413Z","shell.execute_reply.started":"2024-05-20T16:29:01.242508Z","shell.execute_reply":"2024-05-20T16:29:01.245454Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"for i in questions:\n    question = clean_tweet(i) \n    predicted_bloom_level = make_prediction(question,model,tokenizer)\n    output.append(predicted_bloom_level)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:29:03.617423Z","iopub.execute_input":"2024-05-20T16:29:03.618175Z","iopub.status.idle":"2024-05-20T16:29:04.457968Z","shell.execute_reply.started":"2024-05-20T16:29:03.618144Z","shell.execute_reply":"2024-05-20T16:29:04.457059Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:29:07.831155Z","iopub.execute_input":"2024-05-20T16:29:07.831525Z","iopub.status.idle":"2024-05-20T16:29:07.837756Z","shell.execute_reply.started":"2024-05-20T16:29:07.831495Z","shell.execute_reply":"2024-05-20T16:29:07.836867Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[3, 0, 1, 1, 2, 2, 2, 5, 4, 4, 5, 5]"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}}]}